<!DOCTYPE HTML>

<html>
	<head>
		<title>Generic - Solid State by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" type="image/png" href="./favicon.png">
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Ethan Outangoun</a></h1>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<div class="inner">
							<h2>Menu</h2>
							<ul class="links">
								<li><a href="index.html">Home</a></li>
								<li><a href="projects.html">Generic</a></li>
								<li><a href="elements.html">Elements</a></li>
								
							</ul>
							<a href="#" class="close">Close</a>
						</div>
					</nav>

				<!-- Wrapper -->
					<section id="wrapper">
						<header>
							<div class="inner">
								<h2>Olympian Webscraper</h2>
                                <h4>Languages and Tools used: Python, BeautifulSoup, Selenium</h4>
                                <a href="./ws.py" class="button primary icon solid fa-download">Download Python File</a>
                                
								
							</div>
						</header>

						<!-- Content -->
							<div class="wrapper">
								<div class="inner">
                                    <h3 class="major">Purpose</h3>
									<p>This program was used to webscrape faces and names of olympians for the purpose of training an AI model to infer ethnicity. Creates a .csv file containing first name, last name, and link to picture of olympian.</p>
                                   
                                    <h4>Source Code</h4>
										<pre><code>
from bs4 import BeautifulSoup
import csv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

# Important columns in CSV file
IMAGE_ADDRESS_COL = 8
FIRST_NAME_COL = 0
LAST_NAME_COL = 1


def web_scrape():
    # this array stores all the new rows that will be appended to the csv
    outp = []
    header = ['First Name', 'Last Name', 'White', 'Black', 'Asian', 'Other', 'Highest Prob. Score', 'Filler', 'Image']
    # NOTE: Change the destination of the new photos csv file here
    f = open('/Users/ethan/Desktop/Research/Webscrape Olympians/2020-Olympians.csv', 'w+', encoding='UTF8', newline='')
    writer = csv.writer(f)
    writer.writerow(header)

    # link to entire nba players list
    url = "https://olympics.com/en/olympic-games/tokyo-2020/athletes"
    # request information from a URL
    options = webdriver.ChromeOptions()
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    driver = webdriver.Chrome(options=options)
    driver.get(url)
    
    #Accepts cookies
    button = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
    )
    button.click()


    #Expand table
    while(True):
        try:
            button = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'button[data-cy="more-button"]')))
            button.click()
        except:
            break
    time.sleep(5)
    
    # create a BeautifulSoup object 
    # "html.parser" is just one option of content parsers
    soup = BeautifulSoup(driver.page_source, "html.parser")
    

    #retrieve divs containing names and potential images
    np_div = soup.find_all(attrs={'class':'styles__Athlete-sc-1yhe77y-0 gFYhln'})

    for i in range(len(np_div)):
        photo = np_div[i].find("div").find("a").find("picture")
        names = np_div[i].find_all("a")[1].find("div").find("h3").string.split()

        #If doesnt have at least two names, skip
        if len(names)<2:
            continue
        full_name = names[0] + " " + names[len(names)-1]
        
        
        if photo is None:
            continue
        else:
            photo = photo.find("source")['data-srcset']
            photo = photo.split(", ")
            photo = photo[1].split()[0]
            print(full_name, photo)
            outp.append([names[0],names[1], None, None, None, None, None, None, photo])

            
    writer.writerows(outp)
    print(total)
    driver.quit()

def main():
    web_scrape()

if __name__=="__main__":
    main()</code></pre>

				


								</div>
							</div>

					</section>

				<!-- Footer -->
					<section id="footer">
						<div class="inner">
							<h2 class="major">Contact Me</h2>

							
							<ul class="contact">
								
								<li class="icon solid fa-phone">(510) 366-9197</li>
								<li class="icon solid fa-envelope"><a href="mailto:ethanoutangoun@gmail.com">ethanoutangoun@gmail.com</a></li>
								<li href="https://github.com/ethanoutangoun" class="icon brands fa-github"><span class="label">Github</span><a href="https://github.com/ethanoutangoun" target="_blank">github.com/ethanoutangoun</a></li>
                                <li class="icon solid fa-home">
									San Francisco, CA<br />
									
								</li>
								
							</ul>
						
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>